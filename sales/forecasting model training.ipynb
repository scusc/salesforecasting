{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta,date\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = pd.read_csv('date_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.sort_values(by=['date'],inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff =df_sales.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff['prev_sales'] = df_diff['sales'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>prev_sales</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>468.900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7948</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>129.552</td>\n",
       "      <td>468.900</td>\n",
       "      <td>-339.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7946</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>362.250</td>\n",
       "      <td>129.552</td>\n",
       "      <td>232.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7945</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>18.840</td>\n",
       "      <td>362.250</td>\n",
       "      <td>-343.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8307</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>137.352</td>\n",
       "      <td>18.840</td>\n",
       "      <td>118.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>209.300</td>\n",
       "      <td>3.024</td>\n",
       "      <td>206.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>323.136</td>\n",
       "      <td>209.300</td>\n",
       "      <td>113.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>13.904</td>\n",
       "      <td>323.136</td>\n",
       "      <td>-309.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>20.720</td>\n",
       "      <td>13.904</td>\n",
       "      <td>6.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>90.930</td>\n",
       "      <td>20.720</td>\n",
       "      <td>70.210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9789 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date    sales  prev_sales     diff\n",
       "540   2015-01-02  468.900         NaN      NaN\n",
       "7948  2015-01-03  129.552     468.900 -339.348\n",
       "7946  2015-01-03  362.250     129.552  232.698\n",
       "7945  2015-01-03   18.840     362.250 -343.410\n",
       "8307  2015-01-03  137.352      18.840  118.512\n",
       "...          ...      ...         ...      ...\n",
       "645   2018-12-30  209.300       3.024  206.276\n",
       "906   2018-12-30  323.136     209.300  113.836\n",
       "1296  2018-12-30   13.904     323.136 -309.232\n",
       "1297  2018-12-30   20.720      13.904    6.816\n",
       "907   2018-12-30   90.930      20.720   70.210\n",
       "\n",
       "[9789 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diff['diff'] = (df_diff['sales'] - df_diff['prev_sales'])\n",
    "df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervised = df_diff.drop(['prev_sales'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inc in range(1,13):\n",
    "    field_name = 'lag_' + str(inc)\n",
    "    df_supervised[field_name] = df_supervised['diff'].shift(inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervised = df_supervised.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25088670775642974\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# Define the regression formula\n",
    "model = smf.ols(formula='diff ~ lag_1', data=df_supervised)\n",
    "\n",
    "# Fit the regression\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Extract the adjusted r-squared\n",
    "regression_adj_rsq = model_fit.rsquared_adj\n",
    "print(regression_adj_rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4643104328364236\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# Define the regression formula\n",
    "model = smf.ols(formula='diff ~ lag_1 + lag_2 + lag_3 + lag_4 + lag_5 + lag_6 + lag_7 + lag_8 + lag_9 + lag_10 + lag_11 + lag_12', data=df_supervised)\n",
    "\n",
    "# Fit the regression\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Extract the adjusted r-squared\n",
    "regression_adj_rsq = model_fit.rsquared_adj\n",
    "print(regression_adj_rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df_model = df_supervised.drop(['sales','date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = df_model[0:-7].values, df_model[-7:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply Min Max Scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = scaler.fit(train_set)\n",
    "# reshape training set\n",
    "train_set = train_set.reshape(train_set.shape[0], train_set.shape[1])\n",
    "train_set_scaled = scaler.transform(train_set)\n",
    "\n",
    "# reshape test set\n",
    "test_set = test_set.reshape(test_set.shape[0], test_set.shape[1])\n",
    "test_set_scaled = scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1]\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9769/9769 [==============================] - 7s 677us/step - loss: 0.0014\n",
      "Epoch 2/100\n",
      "9769/9769 [==============================] - 6s 583us/step - loss: 9.1624e-04\n",
      "Epoch 3/100\n",
      "9769/9769 [==============================] - 6s 601us/step - loss: 9.0440e-04\n",
      "Epoch 4/100\n",
      "9769/9769 [==============================] - 6s 589us/step - loss: 8.8930e-04\n",
      "Epoch 5/100\n",
      "9769/9769 [==============================] - 6s 635us/step - loss: 8.7775e-04\n",
      "Epoch 6/100\n",
      "9769/9769 [==============================] - 7s 710us/step - loss: 8.6973e-04\n",
      "Epoch 7/100\n",
      "9769/9769 [==============================] - 6s 634us/step - loss: 8.6393e-04\n",
      "Epoch 8/100\n",
      "9769/9769 [==============================] - 6s 633us/step - loss: 8.5959e-04\n",
      "Epoch 9/100\n",
      "9769/9769 [==============================] - 6s 627us/step - loss: 8.5635e-04\n",
      "Epoch 10/100\n",
      "9769/9769 [==============================] - 6s 630us/step - loss: 8.5421e-04\n",
      "Epoch 11/100\n",
      "9769/9769 [==============================] - 6s 631us/step - loss: 8.5273e-04\n",
      "Epoch 12/100\n",
      "9769/9769 [==============================] - 6s 633us/step - loss: 8.5159e-04\n",
      "Epoch 13/100\n",
      "9769/9769 [==============================] - 6s 633us/step - loss: 8.5061e-04\n",
      "Epoch 14/100\n",
      "9769/9769 [==============================] - 6s 620us/step - loss: 8.4973e-04\n",
      "Epoch 15/100\n",
      "9769/9769 [==============================] - 6s 627us/step - loss: 8.4890e-04\n",
      "Epoch 16/100\n",
      "9769/9769 [==============================] - 6s 640us/step - loss: 8.4808e-04\n",
      "Epoch 17/100\n",
      "9769/9769 [==============================] - 6s 639us/step - loss: 8.4724e-04\n",
      "Epoch 18/100\n",
      "9769/9769 [==============================] - 6s 632us/step - loss: 8.4634e-04\n",
      "Epoch 19/100\n",
      "9769/9769 [==============================] - 6s 638us/step - loss: 8.4537e-04\n",
      "Epoch 20/100\n",
      "9769/9769 [==============================] - 6s 634us/step - loss: 8.4432e-04\n",
      "Epoch 21/100\n",
      "9769/9769 [==============================] - 6s 638us/step - loss: 8.4323e-04\n",
      "Epoch 22/100\n",
      "9769/9769 [==============================] - 6s 636us/step - loss: 8.4212e-04\n",
      "Epoch 23/100\n",
      "9769/9769 [==============================] - 6s 637us/step - loss: 8.4104e-04\n",
      "Epoch 24/100\n",
      "9769/9769 [==============================] - 6s 636us/step - loss: 8.4000e-04\n",
      "Epoch 25/100\n",
      "9769/9769 [==============================] - 6s 637us/step - loss: 8.3904e-04\n",
      "Epoch 26/100\n",
      "9769/9769 [==============================] - 6s 638us/step - loss: 8.3815e-04\n",
      "Epoch 27/100\n",
      "9769/9769 [==============================] - 6s 634us/step - loss: 8.3733e-04\n",
      "Epoch 28/100\n",
      "9769/9769 [==============================] - 6s 638us/step - loss: 8.3656e-04 0s - loss: 8.\n",
      "Epoch 29/100\n",
      "9769/9769 [==============================] - 6s 637us/step - loss: 8.3584e-04\n",
      "Epoch 30/100\n",
      "9769/9769 [==============================] - 6s 632us/step - loss: 8.3514e-04\n",
      "Epoch 31/100\n",
      "9769/9769 [==============================] - 6s 641us/step - loss: 8.3446e-04\n",
      "Epoch 32/100\n",
      "9769/9769 [==============================] - 6s 633us/step - loss: 8.3380e-04\n",
      "Epoch 33/100\n",
      "9769/9769 [==============================] - 6s 639us/step - loss: 8.3318e-04\n",
      "Epoch 34/100\n",
      "9769/9769 [==============================] - 6s 624us/step - loss: 8.3259e-04\n",
      "Epoch 35/100\n",
      "9769/9769 [==============================] - 6s 637us/step - loss: 8.3204e-04\n",
      "Epoch 36/100\n",
      "9769/9769 [==============================] - 6s 641us/step - loss: 8.3151e-04\n",
      "Epoch 37/100\n",
      "9769/9769 [==============================] - 6s 638us/step - loss: 8.3100e-04\n",
      "Epoch 38/100\n",
      "9769/9769 [==============================] - 6s 636us/step - loss: 8.3051e-04\n",
      "Epoch 39/100\n",
      "9769/9769 [==============================] - 6s 636us/step - loss: 8.3003e-04\n",
      "Epoch 40/100\n",
      "9769/9769 [==============================] - 6s 639us/step - loss: 8.2956e-04\n",
      "Epoch 41/100\n",
      "9769/9769 [==============================] - 6s 638us/step - loss: 8.2911e-04\n",
      "Epoch 42/100\n",
      "9769/9769 [==============================] - 6s 640us/step - loss: 8.2868e-04\n",
      "Epoch 43/100\n",
      "9769/9769 [==============================] - 7s 683us/step - loss: 8.2826e-04\n",
      "Epoch 44/100\n",
      "9769/9769 [==============================] - 6s 586us/step - loss: 8.2786e-04\n",
      "Epoch 45/100\n",
      "9769/9769 [==============================] - 5s 556us/step - loss: 8.2747e-04\n",
      "Epoch 46/100\n",
      "9769/9769 [==============================] - 6s 644us/step - loss: 8.2710e-04\n",
      "Epoch 47/100\n",
      "9769/9769 [==============================] - 6s 588us/step - loss: 8.2674e-04\n",
      "Epoch 48/100\n",
      "9769/9769 [==============================] - 8s 866us/step - loss: 8.2638e-04\n",
      "Epoch 49/100\n",
      "9769/9769 [==============================] - 6s 613us/step - loss: 8.2603e-04\n",
      "Epoch 50/100\n",
      "9769/9769 [==============================] - 6s 624us/step - loss: 8.2569e-04\n",
      "Epoch 51/100\n",
      "9769/9769 [==============================] - 6s 657us/step - loss: 8.2535e-04\n",
      "Epoch 52/100\n",
      "9769/9769 [==============================] - 7s 697us/step - loss: 8.2501e-04\n",
      "Epoch 53/100\n",
      "9769/9769 [==============================] - 7s 732us/step - loss: 8.2468e-04\n",
      "Epoch 54/100\n",
      "9769/9769 [==============================] - 7s 693us/step - loss: 8.2436e-04\n",
      "Epoch 55/100\n",
      "9769/9769 [==============================] - 6s 648us/step - loss: 8.2405e-04\n",
      "Epoch 56/100\n",
      "9769/9769 [==============================] - 6s 663us/step - loss: 8.2374e-04\n",
      "Epoch 57/100\n",
      "9769/9769 [==============================] - 7s 753us/step - loss: 8.2344e-04\n",
      "Epoch 58/100\n",
      "9769/9769 [==============================] - 7s 699us/step - loss: 8.2316e-04 0s - loss: 8.3658e\n",
      "Epoch 59/100\n",
      "9769/9769 [==============================] - 6s 585us/step - loss: 8.2290e-04\n",
      "Epoch 60/100\n",
      "9769/9769 [==============================] - 7s 666us/step - loss: 8.2265e-04\n",
      "Epoch 61/100\n",
      "9769/9769 [==============================] - 6s 583us/step - loss: 8.2243e-04\n",
      "Epoch 62/100\n",
      "9769/9769 [==============================] - 6s 632us/step - loss: 8.2222e-04\n",
      "Epoch 63/100\n",
      "9769/9769 [==============================] - 7s 722us/step - loss: 8.2202e-04\n",
      "Epoch 64/100\n",
      "9769/9769 [==============================] - 7s 742us/step - loss: 8.2184e-04\n",
      "Epoch 65/100\n",
      "9769/9769 [==============================] - 6s 612us/step - loss: 8.2166e-04\n",
      "Epoch 66/100\n",
      "9769/9769 [==============================] - 7s 737us/step - loss: 8.2150e-04\n",
      "Epoch 67/100\n",
      "9769/9769 [==============================] - 6s 651us/step - loss: 8.2136e-04\n",
      "Epoch 68/100\n",
      "9769/9769 [==============================] - 6s 647us/step - loss: 8.2122e-04\n",
      "Epoch 69/100\n",
      "9769/9769 [==============================] - 6s 651us/step - loss: 8.2109e-04\n",
      "Epoch 70/100\n",
      "9769/9769 [==============================] - 7s 693us/step - loss: 8.2098e-04\n",
      "Epoch 71/100\n",
      "9769/9769 [==============================] - 7s 714us/step - loss: 8.2086e-04\n",
      "Epoch 72/100\n",
      "9769/9769 [==============================] - 6s 602us/step - loss: 8.2076e-04\n",
      "Epoch 73/100\n",
      "9769/9769 [==============================] - 6s 620us/step - loss: 8.2066e-04\n",
      "Epoch 74/100\n",
      "9769/9769 [==============================] - 6s 639us/step - loss: 8.2056e-04\n",
      "Epoch 75/100\n",
      "9769/9769 [==============================] - 6s 629us/step - loss: 8.2047e-04\n",
      "Epoch 76/100\n",
      "9769/9769 [==============================] - 7s 717us/step - loss: 8.2038e-04\n",
      "Epoch 77/100\n",
      "9769/9769 [==============================] - 8s 827us/step - loss: 8.2028e-04\n",
      "Epoch 78/100\n",
      "9769/9769 [==============================] - 7s 752us/step - loss: 8.2019e-04\n",
      "Epoch 79/100\n",
      "9769/9769 [==============================] - 6s 653us/step - loss: 8.2010e-04\n",
      "Epoch 80/100\n",
      "9769/9769 [==============================] - 7s 676us/step - loss: 8.2001e-04\n",
      "Epoch 81/100\n",
      "9769/9769 [==============================] - 6s 613us/step - loss: 8.1991e-04\n",
      "Epoch 82/100\n",
      "9769/9769 [==============================] - 7s 687us/step - loss: 8.1982e-04\n",
      "Epoch 83/100\n",
      "9769/9769 [==============================] - 6s 651us/step - loss: 8.1972e-04\n",
      "Epoch 84/100\n",
      "9769/9769 [==============================] - 6s 663us/step - loss: 8.1962e-04\n",
      "Epoch 85/100\n",
      "9769/9769 [==============================] - 7s 719us/step - loss: 8.1952e-04\n",
      "Epoch 86/100\n",
      "9769/9769 [==============================] - 8s 772us/step - loss: 8.1942e-04\n",
      "Epoch 87/100\n",
      "9769/9769 [==============================] - 7s 666us/step - loss: 8.1931e-04\n",
      "Epoch 88/100\n",
      "9769/9769 [==============================] - 6s 651us/step - loss: 8.1921e-04\n",
      "Epoch 89/100\n",
      "9769/9769 [==============================] - 6s 660us/step - loss: 8.1911e-04\n",
      "Epoch 90/100\n",
      "9769/9769 [==============================] - 6s 623us/step - loss: 8.1901e-04\n",
      "Epoch 91/100\n",
      "9769/9769 [==============================] - 7s 756us/step - loss: 8.1890e-04\n",
      "Epoch 92/100\n",
      "9769/9769 [==============================] - 6s 632us/step - loss: 8.1880e-04\n",
      "Epoch 93/100\n",
      "9769/9769 [==============================] - 6s 630us/step - loss: 8.1870e-04\n",
      "Epoch 94/100\n",
      "9769/9769 [==============================] - 6s 635us/step - loss: 8.1860e-04\n",
      "Epoch 95/100\n",
      "9769/9769 [==============================] - 6s 626us/step - loss: 8.1850e-04\n",
      "Epoch 96/100\n",
      "9769/9769 [==============================] - 6s 626us/step - loss: 8.1841e-04\n",
      "Epoch 97/100\n",
      "9769/9769 [==============================] - 6s 647us/step - loss: 8.1832e-04\n",
      "Epoch 98/100\n",
      "9769/9769 [==============================] - 6s 635us/step - loss: 8.1824e-04\n",
      "Epoch 99/100\n",
      "9769/9769 [==============================] - 6s 653us/step - loss: 8.1817e-04\n",
      "Epoch 100/100\n",
      "9769/9769 [==============================] - 7s 707us/step - loss: 8.1810e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7aa417aa60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\",save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
